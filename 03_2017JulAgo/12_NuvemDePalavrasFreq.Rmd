---
title: "Palavras frequentes"
output: html_document
params:
  ini: 2015
  fim: 2015
  partido: "PT"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

### Critérios informados

```{r, echo=FALSE}
source("..\\02_2017MaiJun\\05_EncodeDecode.R")

# parâmetros
ini <- params$ini
fim <- params$fim
partido <- params$partido

# pacotes
if(!require(tm)) { install.packages('tm') }
if(!require(qdap)) { install.packages('qdap') }
if(!require(RWeka)) { install.packages('RWeka') }

if(!require(tidyverse)) { install.packages('tidyverse') }
if(!require(tidytext)) { install.packages('tidytext') }
if(!require(tidyr)) { install.packages('tidyr') }
if(!require(dplyr)) { install.packages('dplyr') }   

if(!require(wordcloud)) { install.packages('wordcloud') }
if(!require(SnowballC)) { install.packages('SnowballC') }
if(!require(stringr)) { install.packages('stringr') }
if(!require(ggplot2)) { install.packages('ggplot2') }
if(!require(gridExtra)) { install.packages('gridExtra') }

# https://rpubs.com/mlmv/swiftkey


# lê arquivo com todos os discursos sob análise
pasta <- "..\\CorpusRDS\\"
arquivo <- paste0("corpus_", partido, "_", ini, "_", fim, "_limpo.rds")
lista <- readRDS(paste0(pasta, arquivo))
docs <- lista$docs
tfq  <- lista$tfq
freq_tdm <- lista$freq_tdm
bigramas <- lista$bigramas

print(sprintf("Partido: %s (%d - %d)", partido, ini, fim))
print(sprintf("Arquivo: %s", arquivo))

# define pallette de cores
purple_orange <- brewer.pal(10, "PuOr")
# drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

```

## Nuvem de unigramas

```{r, echo = FALSE}
# save default, for resetting...
# def.par <- par(no.readonly = TRUE)

# define divisao grafico
# par(mfrow = c(1, 2))

# create a wordcloud with purple_orange palette
```

#### *tfq - contagem bruta*

```{r, echo = FALSE}
tfq <- tfq[!is.na(tfq$FREQ), ]
wordcloud(tfq$WORD, tfq$FREQ, max.words = 20, colors = purple_orange, random.order=FALSE)
```

#### *freq_tdm - tratamento TfIdf e sparsity*

```{r, echo = FALSE}
freq_tdm <- freq_tdm[!is.na(freq_tdm$FREQ), ]
wordcloud(freq_tdm$WORD, freq_tdm$FREQ, max.words = 20, colors = purple_orange, random.order=FALSE)

# reset to default
# par(def.par)
```

## Freqûencia de unigramas

```{r, echo = FALSE}
### gráfico com os 20 termos mais frequentes
p1 <- tfq[1:20,] %>%
   ggplot(aes(x = reorder(WORD, FREQ), y = FREQ)) +
   geom_col() +
   xlab(NULL) +
   coord_flip() +
   theme(panel.background = element_rect(fill='white', colour='gray')) +
   ylab("Freq") + xlab("Palavra") + ggtitle("Contagem bruta")

p2 <- freq_tdm[1:20,] %>%
   ggplot(aes(x = reorder(WORD, FREQ), y = FREQ)) +
   geom_col() +
   xlab(NULL) +
   coord_flip() +
   theme(panel.background = element_rect(fill='white', colour='gray')) +
   ylab("Freq") + xlab("Palavra") + ggtitle("Tratamento TfIdf e sparsity")

grid.arrange(p1, p2)
```

## Dendograma de unigramas

```{r, echo = FALSE}
# dendogramas
tfq <- tfq[1:20,]
hc <- hclust( dist(tfq$FREQ) )
plot(hc, labels = tfq$WORD, main = "Contagem bruta", xlab = "Palavras")

freq_tdm <- freq_tdm[1:20,]
hc <- hclust( dist(freq_tdm$FREQ) )
plot(hc, labels = freq_tdm$WORD, main = "Tratamento TfIdf e Sparsity", xlab = "Palavras")

```

## Nuvem de bigramas

```{r, echo = FALSE}
# Wordcloud de bigramas
wordcloud(bigramas$bigram, bigramas$n, max.words = 20, colors = purple_orange, random.order=FALSE)
```

## Frequência de bigramas

```{r, echo = FALSE}
bigramas <- bigramas[1:20,]
p3 <- bigramas %>%
   ggplot(aes(x = reorder(bigram, n), y = n)) +
   geom_col() +
   xlab(NULL) +
   coord_flip() +
   theme(panel.background = element_rect(fill='white', colour='gray')) +
   ylab("Freq") + xlab("Bigrama") + ggtitle("Contagem bruta")
p3
```

## Dendograma de bigramas

```{r, echo = FALSE}
# Dendogramas de bigramas
hc <- hclust( dist(bigramas$n) )
plot(hc, labels = bigramas$bigram, main = "Contagem bruta", xlab = "Bigramas")

```

### Observações

A análise de cluster hierárquica usa o conjunto de diferenças para os objetos em cluster. Inicialmente, cada objeto é atribuído ao seu próprio cluster e, em seguida, o algoritmo prossegue iterativamente, em cada etapa, juntando os dois clusters mais semelhantes, continuando até existir apenas um único cluster. Em cada estágio, as distâncias entre clusters são recalculadas pela fórmula de atualização de dissimilaridade de Lance-Williams, de acordo com o método *complete linkage* que encontra clusters similares.

